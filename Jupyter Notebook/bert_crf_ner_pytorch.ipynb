{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "import parmap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchcrf import CRF\n",
    "from torchmetrics import F1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertConfig, BertForTokenClassification\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, sep = '\\t'):\n",
    "    '''    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        dataset directory\n",
    "    sep : TYPE, optional\n",
    "        DESCRIPTION. The default is '\\t'.\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary : TYPE\n",
    "        DESCRIPTION.\n",
    "    this load function is for korean corpus dataset\n",
    "    '''\n",
    "    \n",
    "    file_lists = glob.glob('/'.join([file_path, '*.JSON']))\n",
    "    \n",
    "    dictionary = []\n",
    "    \n",
    "    for file in tqdm(file_lists):\n",
    "        with open(file, 'r', encoding = 'utf-8') as f:\n",
    "            json_file = json.loads(f.read()).get('document')\n",
    "            \n",
    "            for _json in json_file:\n",
    "                _json = list(filter(lambda x: x['ne'] != [], _json.get('sentence')))\n",
    "                for datum in _json:\n",
    "                    ne_dictionary = dict()\n",
    "                    sentence = datum.get('form')\n",
    "                    dic = ((doc.get('form'), doc.get('label')) for doc in datum.get('ne'))\n",
    "                    \n",
    "                    for word, tag in dic:\n",
    "                        ne_dictionary[word] = tag\n",
    "                    \n",
    "                    temp = {'sentence': sentence, 'ne': ne_dictionary}\n",
    "                    \n",
    "                    dictionary.append(temp)\n",
    "                    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(dataset):\n",
    "    \n",
    "    input_dataframe = pd.DataFrame(columns = ['text', 'tags'])\n",
    "    \n",
    "    for data in tqdm(dataset):\n",
    "        sentence = data['sentence']\n",
    "        sentence = sentence.replace('\\xad', '­＿')\n",
    "        name_entity = data['ne']\n",
    "        \n",
    "        if name_entity == {}:\n",
    "            continue\n",
    "        \n",
    "        bert_tokenized_sentence = tokenizer.wordpiece_tokenizer.tokenize(sentence)\n",
    "        sentence = bert_tokenized_sentence\n",
    "        \n",
    "        character_dataframe = pd.DataFrame([j for i in sentence for j in i], columns = ['text'])\n",
    "\n",
    "        try:\n",
    "            for key in name_entity.keys():\n",
    "                no_space_key = key.replace(' ', '')\n",
    "                for find in re.finditer(no_space_key, ''.join(sentence)):                \n",
    "                    index = find.span()\n",
    "                    if ( index[1] - index[0] ) == 1:\n",
    "                        character_dataframe.loc[index[0], 'tag'] = 'B-' + name_entity[key]\n",
    "                    else:\n",
    "                        character_dataframe.loc[index[0], 'tag'] = 'B-' + name_entity[key]\n",
    "                        character_dataframe.loc[( index[0] + 1 ) : (index[1] - 1 ), 'tag'] = 'I-' + name_entity[key]\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        character_dataframe.fillna('O', inplace = True)\n",
    "        \n",
    "        start = 0\n",
    "        bert_tag_list = []\n",
    "        for token in bert_tokenized_sentence:\n",
    "            bert_tag_list.append((token, start, len(token)))\n",
    "            start += len(token)\n",
    "        \n",
    "        try:\n",
    "            temp_dict = [{'name': row[0], 'tag': character_dataframe.iloc[row[1]].tag} for row in bert_tag_list]\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        bert_tag = list(map(lambda x: x['tag'], temp_dict))\n",
    "        \n",
    "        input_dataframe = input_dataframe.append(pd.DataFrame([[sentence, bert_tag]], columns = ['text', 'tags']))\n",
    "        \n",
    "        input_dataframe = input_dataframe[input_dataframe.text.map(len) <= 98]\n",
    "        input_dataframe = input_dataframe[input_dataframe.text.apply(lambda x: max([len(i) for i in x]))  <= 18]\n",
    "        input_dataframe.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "    return input_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            data: pd.DataFrame,\n",
    "            tokenizer: BertTokenizer,\n",
    "            text_max_token_length: int = 256\n",
    "            ):\n",
    "                \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.text_max_token_length = text_max_token_length\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _get_bert_input_data(self, text):\n",
    "                \n",
    "        # truncation\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(text)\n",
    "        if len(input_ids) > (self.text_max_token_length - 2):\n",
    "            input_ids = input_ids[:(self.text_max_token_length - 2)]\n",
    "        input_ids = list(itertools.chain(*[[2], input_ids, [3]]))\n",
    "                    \n",
    "        attention_mask = pad_sequences([[1] * len(input_ids)], maxlen = self.text_max_token_length, padding = 'post')\n",
    "        segment_ids = [[0] * self.text_max_token_length]\n",
    "        \n",
    "        input_ids = pad_sequences([input_ids], maxlen = self.text_max_token_length, padding = 'post', dtype = 'int32')\n",
    "        \n",
    "        return dict(\n",
    "            input_ids = torch.tensor(input_ids), \n",
    "            attention_mask = torch.tensor(attention_mask),\n",
    "            segment_ids = torch.tensor(segment_ids)\n",
    "            )\n",
    "        \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        data_row = self.data.iloc[index]\n",
    "    \n",
    "        encoded_text = self._get_bert_input_data(data_row['text'])\n",
    "        \n",
    "        return dict(\n",
    "            input_ids = encoded_text['input_ids'].flatten().long(),\n",
    "            token_type_ids = encoded_text['segment_ids'].flatten().long(),\n",
    "            attention_mask = encoded_text['attention_mask'].flatten().long(),\n",
    "            label = torch.tensor(data_row.tags)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(            \n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: BertTokenizer,\n",
    "        batch_size: int = 128,\n",
    "        text_max_token_length: int = 256,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_max_token_length = text_max_token_length\n",
    "        \n",
    "        self.setup()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_df)\n",
    "    \n",
    "    \n",
    "    def prepare_data(self):\n",
    "        NERDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_length,\n",
    "            )\n",
    "        \n",
    "    \n",
    "    def setup(self, stage = None):\n",
    "        self.train_dataset = NERDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_length,\n",
    "            )\n",
    "\n",
    "        self.test_dataset = NERDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.text_max_token_length,\n",
    "            )\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):        \n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = False\n",
    "            )\n",
    "\n",
    "    \n",
    "    def val_dataloader(self):        \n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = False\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):        \n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = False\n",
    "            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pytorch_crf_ner(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, train_samples = 1751, batch_size = 128, epochs = 10, num_labels = 306):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.train_samples = train_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.epochs = epochs\n",
    "        self.warm_up_proportion = 0.2\n",
    "        self.num_train_optimization_steps = int(self.train_samples / self.batch_size / self.gradient_accumulation_steps) * epochs\n",
    "        self.num_warmup_steps = int(float(self.num_train_optimization_steps) * self.warm_up_proportion)\n",
    "\n",
    "        config = BertConfig.from_pretrained('model', output_hidden_states = True)\n",
    "        config.num_labels = num_labels\n",
    "        self.bert_model = BertForTokenClassification.from_pretrained('model', config = config)\n",
    "        \n",
    "        self.optimizer_grouped_parameters = self.get_optimizer_grouped_parameters()\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.linear_layer = nn.Linear(in_features = config.hidden_size, out_features = config.num_labels)\n",
    "        self.crf = CRF(num_tags = config.num_labels, batch_first = True)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, labels = None):\n",
    "        outputs = self.bert_model(\n",
    "            input_ids = input_ids,\n",
    "            token_type_ids = token_type_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels = None\n",
    "            )\n",
    "                                \n",
    "        dropout_layer = self.dropout(outputs.hidden_states[-1])\n",
    "        linear_layer = self.linear_layer(dropout_layer)\n",
    "\n",
    "        if labels is not None:\n",
    "            log_likelihood = -self.crf(linear_layer, labels, mask = attention_mask.bool(), reduction = 'token_mean')\n",
    "            sequence_of_tags = self.crf.decode(linear_layer, mask = attention_mask.bool())\n",
    "            \n",
    "            return log_likelihood, sequence_of_tags\n",
    "        \n",
    "        else:\n",
    "            sequence_of_tags = torch.tensor(self.crf.decode(linear_layer, mask = attention_mask.bool())).cuda()\n",
    "            return sequence_of_tags\n",
    "\n",
    "         \n",
    "    def training_step(self, batch, batch_index):\n",
    "        input_ids = batch['input_ids']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        tags = batch['label'].long()\n",
    "\n",
    "        log_likelihood, sequence_of_tags = self(\n",
    "            input_ids = input_ids,\n",
    "            token_type_ids = token_type_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels = tags\n",
    "            )\n",
    "        \n",
    "        sequence_of_tags = torch.tensor(list(itertools.chain(*sequence_of_tags)))\n",
    "        real = tags[attention_mask.bool()].cpu()\n",
    "        correct_num = sum(torch.eq(real, sequence_of_tags))\n",
    "        total_num = attention_mask.bool().sum()        \n",
    "        acc = correct_num / total_num\n",
    "        f1score = f1_score(real.cpu(), sequence_of_tags, average = 'weighted')\n",
    "        \n",
    "        self.log('train_acc', acc, prog_bar = True, logger = True)            \n",
    "        self.log('train_f1', f1score, prog_bar = True, logger = True)            \n",
    "        \n",
    "        return log_likelihood\n",
    "      #  return log_likelihood\n",
    "        \n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_index):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        tags = batch['label'].long()\n",
    "        \n",
    "          \n",
    "        log_likelihood, sequence_of_tags = self(\n",
    "            input_ids = input_ids,\n",
    "            token_type_ids = token_type_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels = tags\n",
    "            )\n",
    "        \n",
    "        sequence_of_tags = torch.tensor(list(itertools.chain(*sequence_of_tags)))\n",
    "        real = tags[attention_mask.bool()].cpu()\n",
    "        correct_num = sum(torch.eq(real, sequence_of_tags))\n",
    "        total_num = attention_mask.bool().sum()        \n",
    "        acc = correct_num / total_num\n",
    "        f1score = f1_score(real.cpu(), sequence_of_tags, average = 'weighted')\n",
    "        \n",
    "        metrics = {'val_f1': f1score, 'val_acc': acc, 'val_loss': log_likelihood}\n",
    "         \n",
    "        self.log_dict(metrics)\n",
    "        \n",
    "        return {'val_loss': log_likelihood}\n",
    "        \n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_index):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        tags = batch['label']\n",
    "\n",
    "        linear_layer, sequence_of_tags = self.bert_model(\n",
    "            input_ids = input_ids,\n",
    "            token_type_ids = token_type_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            # labels = tags\n",
    "            )\n",
    "                \n",
    "        sequence_of_tags = torch.tensor(list(itertools.chain(*sequence_of_tags)))\n",
    "        real = tags[attention_mask.bool()].cpu()\n",
    "        correct_num = sum(torch.eq(real, sequence_of_tags))\n",
    "        total_num = attention_mask.bool().sum()        \n",
    "        acc = correct_num / total_num\n",
    "        f1score = f1_score(real.cpu(), sequence_of_tags, average = 'weighted')\n",
    "        \n",
    "        metrics = {'val_f1': f1score, 'val_acc': acc, 'val_loss': log_likelihood}\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "     \n",
    "    def get_optimizer_grouped_parameters(self):\n",
    "                \n",
    "        no_decay_layer_list = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        \n",
    "        optimizer_grouped_parameters = []\n",
    "\n",
    "        layers = list(self.bert_model.named_parameters())\n",
    "        \n",
    "        encoder_decay = {\n",
    "            'params': [param for name, param in layers if\n",
    "                       not any(no_decay_layer_name in name for no_decay_layer_name in no_decay_layer_list)],\n",
    "            'weight_decay': 0.01\n",
    "            }\n",
    "    \n",
    "        encoder_nodecay = {\n",
    "            'params': [param for name, param in layers if\n",
    "                       any(no_decay_layer_name in name for no_decay_layer_name in no_decay_layer_list)],\n",
    "            'weight_decay': 0.0}\n",
    "        \n",
    "        optimizer_grouped_parameters.append(encoder_decay)\n",
    "        optimizer_grouped_parameters.append(encoder_nodecay)\n",
    "            \n",
    "        return optimizer_grouped_parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            self.optimizer_grouped_parameters,            \n",
    "            correct_bias = False\n",
    "            )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps = self.num_warmup_steps,\n",
    "            num_training_steps = self.num_train_optimization_steps\n",
    "            )\n",
    "        \n",
    "        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:56<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer('vocab.korean.rawtext.list')\n",
    "dataset = load_data('dataset/corpus_korean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_dataset = np.array_split(dataset[:200000], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [10:28<00:00, 19.90it/s]\n",
      "100%|██████████| 12500/12500 [10:32<00:00, 19.75it/s]\n",
      "100%|██████████| 12500/12500 [10:39<00:00, 19.55it/s]\n",
      "100%|██████████| 12500/12500 [10:47<00:00, 19.31it/s]\n",
      "100%|██████████| 12500/12500 [10:52<00:00, 19.16it/s]\n",
      "100%|██████████| 12500/12500 [10:57<00:00, 19.02it/s]\n",
      "100%|██████████| 12500/12500 [10:58<00:00, 18.99it/s]\n",
      "100%|██████████| 12500/12500 [10:59<00:00, 18.96it/s]\n",
      "100%|██████████| 12500/12500 [11:08<00:00, 18.71it/s]\n",
      "100%|██████████| 12500/12500 [11:11<00:00, 18.61it/s]\n",
      "100%|██████████| 12500/12500 [11:12<00:00, 18.59it/s]\n",
      "100%|██████████| 12500/12500 [11:13<00:00, 18.55it/s]\n",
      "100%|██████████| 12500/12500 [11:22<00:00, 18.32it/s]\n",
      "100%|██████████| 12500/12500 [11:24<00:00, 18.26it/s]\n",
      "100%|██████████| 12500/12500 [11:28<00:00, 18.14it/s]\n",
      "100%|██████████| 12500/12500 [11:29<00:00, 18.13it/s]\n",
      "100%|██████████| 16/16 [11:30<00:00, 43.16s/it]\n"
     ]
    }
   ],
   "source": [
    "temp_input_dataframe = parmap.map(get_input_data, splitted_dataset, pm_pbar = True, pm_processes = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataframe = pd.DataFrame()\n",
    "for dataframe in temp_input_dataframe:\n",
    "    input_dataframe = input_dataframe.append(dataframe)\n",
    "    \n",
    "input_dataframe.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198780/198780 [00:01<00:00, 165211.00it/s]\n",
      "100%|██████████| 198780/198780 [00:00<00:00, 296593.23it/s]\n",
      "100%|██████████| 198780/198780 [00:05<00:00, 37693.62it/s]\n"
     ]
    }
   ],
   "source": [
    "'''for corpus korean'''\n",
    "tags = set(itertools.chain(*[set(i.values()) for i in list(map(lambda x: x['ne'], dataset))]))\n",
    "tags = list(map(lambda x: 'B-'+ x, tags)) + list(map(lambda x: 'I-' + x, tags))\n",
    "\n",
    "tags_to_ids = {c: (i + 5) for i, c in enumerate(tags)}\n",
    "tags_to_ids['O'] = 305\n",
    "tags_to_ids['[PAD]'] = 0\n",
    "tags_to_ids['[UNK]'] = 1\n",
    "tags_to_ids['[CLS]'] = 2\n",
    "tags_to_ids['[SEP]'] = 3\n",
    "tags_to_ids['[MASK]'] = 4\n",
    "ids_to_tags = {}\n",
    "for key, value in tags_to_ids.items():\n",
    "    ids_to_tags[value] = key\n",
    "\n",
    "input_dataframe.tags = input_dataframe.tags.progress_apply(lambda x: ['[CLS]'] + x + ['[SEP]'])\n",
    "input_dataframe.tags = input_dataframe.tags.progress_apply(lambda x: [tags_to_ids[i] for i in x])\n",
    "input_dataframe.tags = input_dataframe.tags.progress_apply(lambda x: pad_sequences([x], maxlen = 256, padding = 'post', value = 0)[0])\n",
    "\n",
    "train, test = train_test_split(input_dataframe, test_size = 0.2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = NERDataModule(train, test, tokenizer, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "data_module = NERDataModule(train, test, tokenizer, batch_size = BATCH_SIZE)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = 'checkpoints',\n",
    "    filename = 'best-checkpoint',\n",
    "    save_top_k = 1,\n",
    "    verbose = True,\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min'\n",
    "    )\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 2,\n",
    "    mode = 'min'\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger('lightning_logs', name = 'ner')\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger = logger,\n",
    "    callbacks = [checkpoint_callback, early_stopping],\n",
    "    max_epochs = EPOCHS,\n",
    "    progress_bar_refresh_rate = 1,\n",
    "    gpus = 2,\n",
    "    accelerator = 'dp'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name         | Type                       | Params\n",
      "------------------------------------------------------------\n",
      "0 | bert_model   | BertForTokenClassification | 109 M \n",
      "1 | dropout      | Dropout                    | 0     \n",
      "2 | linear_layer | Linear                     | 235 K \n",
      "3 | crf          | CRF                        | 94.2 K\n",
      "------------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "438.671   Total estimated model params size (MB)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3abb940a25418287d0a1e57c97faa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1242: val_loss reached 0.31895 (best 0.31895), saving model to \"/home/ubuntu/Documents/python_code/ner/checkpoints/best-checkpoint.ckpt\" as top 1\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, step 2485: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, step 3728: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pytorch_crf_ner(train_samples = 600000, batch_size = BATCH_SIZE, epochs = EPOCHS, num_labels = len(tags_to_ids))\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "ner_model = pytorch_crf_ner.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "ner_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityExtractor:\n",
    "    \n",
    "    def __init__(self, model, tokenizer, max_len):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.IDS_TO_TAGS = dict(        \n",
    "            {\n",
    "                0: '[PAD]',\n",
    "                1: '[UNK]',\n",
    "                2: '[CLS]',\n",
    "                3: '[SEP]',\n",
    "                4: '[MASK]',\n",
    "                5: 'B-OGG_RELIGION',\n",
    "                6: 'B-QT_TEMPERATURE',\n",
    "                7: 'B-PT_FRUIT',\n",
    "                8: 'B-AFA_VIDEO',\n",
    "                9: 'B-OGG_ART',\n",
    "                10: 'B-AM_OTHERS',\n",
    "                11: 'B-MT_ROCK',\n",
    "                12: 'B-TM_SPORTS',\n",
    "                13: 'B-CV_CURRENCY',\n",
    "                14: 'B-LC_SPACE',\n",
    "                15: 'B-AF_WEAPON',\n",
    "                16: 'B-AFA_DOCUMENT',\n",
    "                17: 'B-LCG_OCEAN',\n",
    "                18: 'B-AFW_OTHER_PRODUCTS',\n",
    "                19: 'B-QT_MAN_COUNT',\n",
    "                20: 'B-TMI_SW',\n",
    "                21: 'B-TI_MINUTE',\n",
    "                22: 'B-TMI_MODEL',\n",
    "                23: 'B-QT_CHANNEL',\n",
    "                24: 'B-CV_FOOD',\n",
    "                25: 'B-TR_SCIENCE',\n",
    "                26: 'B-TMI_SITE',\n",
    "                27: 'B-PT_GRASS',\n",
    "                28: 'B-PS_NAME',\n",
    "                29: 'B-TM_SHAPE',\n",
    "                30: 'B-OGG_POLITICS',\n",
    "                31: 'B-EV_FESTIVAL',\n",
    "                32: 'B-DT_OTHERS',\n",
    "                33: 'B-TM_DIRECTION',\n",
    "                34: 'B-OGG_FOOD',\n",
    "                35: 'B-CV_FOOD_STYLE',\n",
    "                36: 'B-DT_SEASON',\n",
    "                37: 'B-CV_TAX',\n",
    "                38: 'B-FD_MEDICINE',\n",
    "                39: 'B-AFA_PERFORMANCE',\n",
    "                40: 'B-OGG_MEDIA',\n",
    "                41: 'B-MT_CHEMICAL',\n",
    "                42: 'B-CV_CLOTHING',\n",
    "                43: 'B-CV_SPORTS_POSITION',\n",
    "                44: 'B-CV_ART',\n",
    "                45: 'B-FD_SOCIAL_SCIENCE',\n",
    "                46: 'B-TR_MEDICINE',\n",
    "                47: 'B-QT_SPEED',\n",
    "                48: 'B-QT_OTHERS',\n",
    "                49: 'B-CV_BUILDING_TYPE',\n",
    "                50: 'B-TM_CELL_TISSUE_ORGAN',\n",
    "                51: 'B-PT_TYPE',\n",
    "                52: 'B-TI_OTHERS',\n",
    "                53: 'B-QT_PHONE',\n",
    "                54: 'B-LCP_PROVINCE',\n",
    "                55: 'B-CV_LANGUAGE',\n",
    "                56: 'B-LC_OTHERS',\n",
    "                57: 'B-AF_TRANSPORT',\n",
    "                58: 'B-CV_POLICY',\n",
    "                59: 'B-FD_OTHERS',\n",
    "                60: 'B-MT_METAL',\n",
    "                61: 'B-QT_PRICE',\n",
    "                62: 'B-AF_BUILDING',\n",
    "                63: 'B-DT_MONTH',\n",
    "                64: 'B-TM_COLOR',\n",
    "                65: 'B-EV_SPORTS',\n",
    "                66: 'B-AM_PART',\n",
    "                67: 'B-QT_LENGTH',\n",
    "                68: 'B-MT_ELEMENT',\n",
    "                69: 'B-TMI_HW',\n",
    "                70: 'B-QT_PERCENTAGE',\n",
    "                71: 'B-QT_VOLUME',\n",
    "                72: 'B-CV_LAW',\n",
    "                73: 'B-LCG_MOUNTAIN',\n",
    "                74: 'B-TR_SOCIAL_SCIENCE',\n",
    "                75: 'B-AM_TYPE',\n",
    "                76: 'B-FD_ART',\n",
    "                77: 'B-DT_YEAR',\n",
    "                78: 'B-OGG_LIBRARY',\n",
    "                79: 'B-PT_TREE',\n",
    "                80: 'B-TR_OTHERS',\n",
    "                81: 'B-OGG_OTHERS',\n",
    "                82: 'B-OGG_MILITARY',\n",
    "                83: 'B-FD_SCIENCE',\n",
    "                84: 'B-OGG_LAW',\n",
    "                85: 'B-AM_MAMMALIA',\n",
    "                86: 'B-TR_HUMANITIES',\n",
    "                87: 'B-CV_OCCUPATION',\n",
    "                88: 'B-OGG_HOTEL',\n",
    "                89: 'B-QT_COUNT',\n",
    "                90: 'B-AM_INSECT',\n",
    "                91: 'B-PS_PET',\n",
    "                92: 'B-PT_PART',\n",
    "                93: 'B-QT_WEIGHT',\n",
    "                94: 'B-PS_CHARACTER',\n",
    "                95: 'B-TI_HOUR',\n",
    "                96: 'B-TR_ART',\n",
    "                97: 'B-CV_SPORTS',\n",
    "                98: 'B-QT_ORDER',\n",
    "                99: 'B-EV_OTHERS',\n",
    "                100: 'B-TMM_DRUG',\n",
    "                101: 'B-AF_MUSICAL_INSTRUMENT',\n",
    "                102: 'B-CV_RELATION',\n",
    "                103: 'B-AF_ROAD',\n",
    "                104: 'B-CV_POSITION',\n",
    "                105: 'B-DT_DYNASTY',\n",
    "                106: 'B-OGG_SCIENCE',\n",
    "                107: 'B-CV_CULTURE',\n",
    "                108: 'B-QT_SPORTS',\n",
    "                109: 'B-DT_WEEK',\n",
    "                110: 'B-TI_SECOND',\n",
    "                111: 'B-PT_OTHERS',\n",
    "                112: 'B-CV_DRINK',\n",
    "                113: 'B-CV_PRIZE',\n",
    "                114: 'B-CV_FUNDS',\n",
    "                115: 'B-TMI_EMAIL',\n",
    "                116: 'B-OGG_ECONOMY',\n",
    "                117: 'B-LCP_COUNTY',\n",
    "                118: 'B-CV_TRIBE',\n",
    "                119: 'B-QT_AGE',\n",
    "                120: 'B-AFA_ART_CRAFT',\n",
    "                121: 'B-TM_CLIMATE',\n",
    "                122: 'B-LCP_CAPITALCITY',\n",
    "                123: 'B-LCG_ISLAND',\n",
    "                124: 'B-AFW_SERVICE_PRODUCTS',\n",
    "                125: 'B-QT_ALBUM',\n",
    "                126: 'B-AFA_MUSIC',\n",
    "                127: 'B-PT_FLOWER',\n",
    "                128: 'B-AM_BIRD',\n",
    "                129: 'B-OGG_EDUCATION',\n",
    "                130: 'B-LCG_CONTINENT',\n",
    "                131: 'B-AM_AMPHIBIA',\n",
    "                132: 'B-DT_DURATION',\n",
    "                133: 'B-EV_ACTIVITY',\n",
    "                134: 'B-AF_CULTURAL_ASSET',\n",
    "                135: 'B-LCP_CITY',\n",
    "                136: 'B-OGG_MEDICINE',\n",
    "                137: 'B-TI_DURATION',\n",
    "                138: 'B-LCP_COUNTRY',\n",
    "                139: 'B-LCG_RIVER',\n",
    "                140: 'B-CV_SPORTS_INST',\n",
    "                141: 'B-AM_REPTILIA',\n",
    "                142: 'B-OGG_SPORTS',\n",
    "                143: 'B-TMI_SERVICE',\n",
    "                144: 'B-QT_ADDRESS',\n",
    "                145: 'B-LCG_BAY',\n",
    "                146: 'B-TMI_PROJECT',\n",
    "                147: 'B-QT_SIZE',\n",
    "                148: 'B-DT_DAY',\n",
    "                149: 'B-TMM_DISEASE',\n",
    "                150: 'B-DT_GEOAGE',\n",
    "                151: 'B-FD_HUMANITIES',\n",
    "                152: 'B-AM_FISH',\n",
    "                153: 'B-TMIG_GENRE',\n",
    "                154: 'B-EV_WAR_REVOLUTION',\n",
    "                155: 'I-OGG_RELIGION',\n",
    "                156: 'I-QT_TEMPERATURE',\n",
    "                157: 'I-PT_FRUIT',\n",
    "                158: 'I-AFA_VIDEO',\n",
    "                159: 'I-OGG_ART',\n",
    "                160: 'I-AM_OTHERS',\n",
    "                161: 'I-MT_ROCK',\n",
    "                162: 'I-TM_SPORTS',\n",
    "                163: 'I-CV_CURRENCY',\n",
    "                164: 'I-LC_SPACE',\n",
    "                165: 'I-AF_WEAPON',\n",
    "                166: 'I-AFA_DOCUMENT',\n",
    "                167: 'I-LCG_OCEAN',\n",
    "                168: 'I-AFW_OTHER_PRODUCTS',\n",
    "                169: 'I-QT_MAN_COUNT',\n",
    "                170: 'I-TMI_SW',\n",
    "                171: 'I-TI_MINUTE',\n",
    "                172: 'I-TMI_MODEL',\n",
    "                173: 'I-QT_CHANNEL',\n",
    "                174: 'I-CV_FOOD',\n",
    "                175: 'I-TR_SCIENCE',\n",
    "                176: 'I-TMI_SITE',\n",
    "                177: 'I-PT_GRASS',\n",
    "                178: 'I-PS_NAME',\n",
    "                179: 'I-TM_SHAPE',\n",
    "                180: 'I-OGG_POLITICS',\n",
    "                181: 'I-EV_FESTIVAL',\n",
    "                182: 'I-DT_OTHERS',\n",
    "                183: 'I-TM_DIRECTION',\n",
    "                184: 'I-OGG_FOOD',\n",
    "                185: 'I-CV_FOOD_STYLE',\n",
    "                186: 'I-DT_SEASON',\n",
    "                187: 'I-CV_TAX',\n",
    "                188: 'I-FD_MEDICINE',\n",
    "                189: 'I-AFA_PERFORMANCE',\n",
    "                190: 'I-OGG_MEDIA',\n",
    "                191: 'I-MT_CHEMICAL',\n",
    "                192: 'I-CV_CLOTHING',\n",
    "                193: 'I-CV_SPORTS_POSITION',\n",
    "                194: 'I-CV_ART',\n",
    "                195: 'I-FD_SOCIAL_SCIENCE',\n",
    "                196: 'I-TR_MEDICINE',\n",
    "                197: 'I-QT_SPEED',\n",
    "                198: 'I-QT_OTHERS',\n",
    "                199: 'I-CV_BUILDING_TYPE',\n",
    "                200: 'I-TM_CELL_TISSUE_ORGAN',\n",
    "                201: 'I-PT_TYPE',\n",
    "                202: 'I-TI_OTHERS',\n",
    "                203: 'I-QT_PHONE',\n",
    "                204: 'I-LCP_PROVINCE',\n",
    "                205: 'I-CV_LANGUAGE',\n",
    "                206: 'I-LC_OTHERS',\n",
    "                207: 'I-AF_TRANSPORT',\n",
    "                208: 'I-CV_POLICY',\n",
    "                209: 'I-FD_OTHERS',\n",
    "                210: 'I-MT_METAL',\n",
    "                211: 'I-QT_PRICE',\n",
    "                212: 'I-AF_BUILDING',\n",
    "                213: 'I-DT_MONTH',\n",
    "                214: 'I-TM_COLOR',\n",
    "                215: 'I-EV_SPORTS',\n",
    "                216: 'I-AM_PART',\n",
    "                217: 'I-QT_LENGTH',\n",
    "                218: 'I-MT_ELEMENT',\n",
    "                219: 'I-TMI_HW',\n",
    "                220: 'I-QT_PERCENTAGE',\n",
    "                221: 'I-QT_VOLUME',\n",
    "                222: 'I-CV_LAW',\n",
    "                223: 'I-LCG_MOUNTAIN',\n",
    "                224: 'I-TR_SOCIAL_SCIENCE',\n",
    "                225: 'I-AM_TYPE',\n",
    "                226: 'I-FD_ART',\n",
    "                227: 'I-DT_YEAR',\n",
    "                228: 'I-OGG_LIBRARY',\n",
    "                229: 'I-PT_TREE',\n",
    "                230: 'I-TR_OTHERS',\n",
    "                231: 'I-OGG_OTHERS',\n",
    "                232: 'I-OGG_MILITARY',\n",
    "                233: 'I-FD_SCIENCE',\n",
    "                234: 'I-OGG_LAW',\n",
    "                235: 'I-AM_MAMMALIA',\n",
    "                236: 'I-TR_HUMANITIES',\n",
    "                237: 'I-CV_OCCUPATION',\n",
    "                238: 'I-OGG_HOTEL',\n",
    "                239: 'I-QT_COUNT',\n",
    "                240: 'I-AM_INSECT',\n",
    "                241: 'I-PS_PET',\n",
    "                242: 'I-PT_PART',\n",
    "                243: 'I-QT_WEIGHT',\n",
    "                244: 'I-PS_CHARACTER',\n",
    "                245: 'I-TI_HOUR',\n",
    "                246: 'I-TR_ART',\n",
    "                247: 'I-CV_SPORTS',\n",
    "                248: 'I-QT_ORDER',\n",
    "                249: 'I-EV_OTHERS',\n",
    "                250: 'I-TMM_DRUG',\n",
    "                251: 'I-AF_MUSICAL_INSTRUMENT',\n",
    "                252: 'I-CV_RELATION',\n",
    "                253: 'I-AF_ROAD',\n",
    "                254: 'I-CV_POSITION',\n",
    "                255: 'I-DT_DYNASTY',\n",
    "                256: 'I-OGG_SCIENCE',\n",
    "                257: 'I-CV_CULTURE',\n",
    "                258: 'I-QT_SPORTS',\n",
    "                259: 'I-DT_WEEK',\n",
    "                260: 'I-TI_SECOND',\n",
    "                261: 'I-PT_OTHERS',\n",
    "                262: 'I-CV_DRINK',\n",
    "                263: 'I-CV_PRIZE',\n",
    "                264: 'I-CV_FUNDS',\n",
    "                265: 'I-TMI_EMAIL',\n",
    "                266: 'I-OGG_ECONOMY',\n",
    "                267: 'I-LCP_COUNTY',\n",
    "                268: 'I-CV_TRIBE',\n",
    "                269: 'I-QT_AGE',\n",
    "                270: 'I-AFA_ART_CRAFT',\n",
    "                271: 'I-TM_CLIMATE',\n",
    "                272: 'I-LCP_CAPITALCITY',\n",
    "                273: 'I-LCG_ISLAND',\n",
    "                274: 'I-AFW_SERVICE_PRODUCTS',\n",
    "                275: 'I-QT_ALBUM',\n",
    "                276: 'I-AFA_MUSIC',\n",
    "                277: 'I-PT_FLOWER',\n",
    "                278: 'I-AM_BIRD',\n",
    "                279: 'I-OGG_EDUCATION',\n",
    "                280: 'I-LCG_CONTINENT',\n",
    "                281: 'I-AM_AMPHIBIA',\n",
    "                282: 'I-DT_DURATION',\n",
    "                283: 'I-EV_ACTIVITY',\n",
    "                284: 'I-AF_CULTURAL_ASSET',\n",
    "                285: 'I-LCP_CITY',\n",
    "                286: 'I-OGG_MEDICINE',\n",
    "                287: 'I-TI_DURATION',\n",
    "                288: 'I-LCP_COUNTRY',\n",
    "                289: 'I-LCG_RIVER',\n",
    "                290: 'I-CV_SPORTS_INST',\n",
    "                291: 'I-AM_REPTILIA',\n",
    "                292: 'I-OGG_SPORTS',\n",
    "                293: 'I-TMI_SERVICE',\n",
    "                294: 'I-QT_ADDRESS',\n",
    "                295: 'I-LCG_BAY',\n",
    "                296: 'I-TMI_PROJECT',\n",
    "                297: 'I-QT_SIZE',\n",
    "                298: 'I-DT_DAY',\n",
    "                299: 'I-TMM_DISEASE',\n",
    "                300: 'I-DT_GEOAGE',\n",
    "                301: 'I-FD_HUMANITIES',\n",
    "                302: 'I-AM_FISH',\n",
    "                303: 'I-TMIG_GENRE',\n",
    "                304: 'I-EV_WAR_REVOLUTION',\n",
    "                305: 'O'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "    def get_input_data(self, dataset):\n",
    "        \n",
    "        input_dataframe = pd.DataFrame(columns = ['text', 'tags'])\n",
    "        \n",
    "        for data in tqdm(dataset):\n",
    "            sentence = data['sentence']\n",
    "            sentence = sentence.replace('\\xad', '­＿')\n",
    "            name_entity = data['ne']\n",
    "            \n",
    "            if name_entity == {}:\n",
    "                continue\n",
    "            \n",
    "            bert_tokenized_sentence = self.tokenizer.wordpiece_tokenizer.tokenize(sentence)\n",
    "            sentence = bert_tokenized_sentence\n",
    "            \n",
    "            character_dataframe = pd.DataFrame([j for i in sentence for j in i], columns = ['text'])\n",
    "            \n",
    "            try:\n",
    "                for key in name_entity.keys():\n",
    "                    no_space_key = key.replace(' ', '')\n",
    "                    for find in re.finditer(no_space_key, ''.join(sentence)):                \n",
    "                        index = find.span()\n",
    "                        if ( index[1] - index[0] ) == 1:\n",
    "                            character_dataframe.loc[index[0], 'tag'] = 'B-' + name_entity[key]\n",
    "                        else:\n",
    "                            character_dataframe.loc[index[0], 'tag'] = 'B-' + name_entity[key]\n",
    "                            character_dataframe.loc[( index[0] + 1 ) : (index[1] - 1 ), 'tag'] = 'I-' + name_entity[key]\n",
    "            \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            character_dataframe.fillna('O', inplace = True)\n",
    "            \n",
    "            start = 0\n",
    "            bert_tag_list = []\n",
    "            for token in bert_tokenized_sentence:\n",
    "                bert_tag_list.append((token, start, len(token)))\n",
    "                start += len(token)\n",
    "            \n",
    "            try:\n",
    "                temp_dict = [{'name': row[0], 'tag': character_dataframe.iloc[row[1]].tag} for row in bert_tag_list]\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            bert_tag = list(map(lambda x: x['tag'], temp_dict))\n",
    "            \n",
    "            input_dataframe = input_dataframe.append(pd.DataFrame([[sentence, bert_tag]], columns = ['text', 'tags']))\n",
    "            \n",
    "            input_dataframe = input_dataframe[input_dataframe.text.map(len) <= ( self.max_len - 2 )]\n",
    "            input_dataframe = input_dataframe[input_dataframe.text.apply(lambda x: max([len(i) for i in x]))  <= 18]\n",
    "            input_dataframe.reset_index(drop = True, inplace = True)\n",
    "            \n",
    "        return input_dataframe\n",
    "    \n",
    "    \n",
    "    def get_bert_input_token(self, text):\n",
    "        \n",
    "        text = self.tokenizer.wordpiece_tokenizer.tokenize(text)\n",
    "        \n",
    "        # truncation\n",
    "        if len(text) > (self.max_len - 2):\n",
    "            text = text[:(self.max_len - 2)]\n",
    "        text.insert(0, '[CLS]')\n",
    "        text += ['[SEP]']\n",
    "        \n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(text)\n",
    "        attention_mask = pad_sequences([[1] * len(input_ids)], maxlen = self.max_len, padding = 'post')\n",
    "        token_type_ids = [[0] * self.max_len]\n",
    "        \n",
    "        input_ids = pad_sequences([input_ids], maxlen = self.max_len, padding = 'post', dtype = 'int32')\n",
    "        \n",
    "        return dict(\n",
    "            input_ids = torch.tensor(input_ids).long(),\n",
    "            token_type_ids = torch.tensor(token_type_ids).long(),\n",
    "            attention_mask = torch.tensor(attention_mask).long()\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def get_entity(self, text):\n",
    "            \n",
    "        input_data = self.get_bert_input_token(text)\n",
    "        predicted_ner = self.model(**input_data)\n",
    "        \n",
    "        text_token = input_data['input_ids'][0][input_data['input_ids'][0] > 0][1:-1]\n",
    "        label_tokens = predicted_ner[0][1:-1].cpu()\n",
    "        \n",
    "        label = list([self.IDS_TO_TAGS[label_token.item()] for label_token in label_tokens])\n",
    "        token_text = self.tokenizer.convert_ids_to_tokens(text_token.tolist())\n",
    "        \n",
    "        entity_list = list(zip(token_text, label))\n",
    "        entity_list = list(map(lambda x: list(x), entity_list))\n",
    "    \n",
    "        for idx, temp_list in enumerate(entity_list):\n",
    "            if idx >= len(entity_list) - 1:\n",
    "                break\n",
    "            \n",
    "            # remove impossible cases\n",
    "            if entity_list[idx][1].startswith('O') and entity_list[idx + 1][1].startswith('I'):\n",
    "                entity_list[idx + 1][1] = 'O'\n",
    "    \n",
    "        \n",
    "        ner_dataframe = pd.DataFrame(columns = ['word', 'entity'])\n",
    "        \n",
    "        last_entity = entity_list[0][0]\n",
    "        index = 0\n",
    "        \n",
    "    \n",
    "        for idx, temp_dict in enumerate(entity_list):\n",
    "            key, value = temp_dict\n",
    "                \n",
    "            if idx >= 1:\n",
    "                last_entity = entity_list[( idx - 1 )][-1]\n",
    "            \n",
    "            key_len = len(key)\n",
    "                    \n",
    "            if text[index: (index + 1)] == ' ':\n",
    "                index += 1\n",
    "                try:\n",
    "                    end_idx += 1\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                if key == text[index: (index + key_len)]:\n",
    "                    previous_index = index\n",
    "                    index += key_len\n",
    "        \n",
    "            else:\n",
    "                if key == text[index: (index + key_len)]:\n",
    "                    previous_index = index\n",
    "                    index += key_len        \n",
    "                else:\n",
    "                    previous_index += key_len\n",
    "                    index += key_len\n",
    "     \n",
    "        \n",
    "            if idx == 0 and value.startswith('B'):\n",
    "                start_idx = previous_index\n",
    "                end_idx = index\n",
    "            \n",
    "            if last_entity.startswith('B') and value.startswith('B'):\n",
    "                word = text[start_idx: end_idx]\n",
    "                ner_dataframe = ner_dataframe.append(pd.DataFrame({'word': word, 'entity': last_entity[2:]}, index = [0]))\n",
    "                \n",
    "                start_idx = previous_index\n",
    "                end_idx = index\n",
    "                \n",
    "            if last_entity.startswith('I') and value.startswith('B'):\n",
    "                word = text[start_idx: end_idx]\n",
    "                ner_dataframe = ner_dataframe.append(pd.DataFrame({'word': word, 'entity': last_entity[2:]}, index = [0]))\n",
    "                \n",
    "                start_idx = previous_index\n",
    "                end_idx = index\n",
    "                        \n",
    "            if last_entity.startswith('O') and value.startswith('B'):\n",
    "                start_idx = previous_index\n",
    "                end_idx = index\n",
    "                \n",
    "            if last_entity.startswith('B') and value.startswith('I'):\n",
    "                end_idx += key_len\n",
    "                \n",
    "            if last_entity.startswith('I') and value.startswith('I'):\n",
    "                end_idx += key_len\n",
    "                \n",
    "            if last_entity.startswith('I') and value.startswith('O'):\n",
    "                word = text[start_idx: end_idx]\n",
    "                ner_dataframe = ner_dataframe.append(pd.DataFrame({'word': word, 'entity': last_entity[2:]}, index = [0]))\n",
    "        \n",
    "        \n",
    "        return ner_dataframe.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_extractor = EntityExtractor(ner_model, tokenizer, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '오늘 서울대학교 전자통신공학부 이재용 학과장과 함께 글로벌 기술 동향을 체크하는 시간을 갖도록 하겠습니다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'B-DT_DAY'),\n",
       " ('서울', 'B-OGG_EDUCATION'),\n",
       " ('대학교', 'I-OGG_EDUCATION'),\n",
       " ('전자', 'O'),\n",
       " ('통신', 'O'),\n",
       " ('공', 'O'),\n",
       " ('학', 'O'),\n",
       " ('부', 'O'),\n",
       " ('이재', 'B-PS_NAME'),\n",
       " ('용', 'I-PS_NAME'),\n",
       " ('학', 'B-CV_POSITION'),\n",
       " ('과', 'I-CV_POSITION'),\n",
       " ('장', 'I-CV_POSITION'),\n",
       " ('과', 'O'),\n",
       " ('함', 'O'),\n",
       " ('께', 'O'),\n",
       " ('글로', 'O'),\n",
       " ('벌', 'O'),\n",
       " ('기술', 'O'),\n",
       " ('동', 'O'),\n",
       " ('향', 'O'),\n",
       " ('을', 'O'),\n",
       " ('체', 'O'),\n",
       " ('크', 'O'),\n",
       " ('하는', 'O'),\n",
       " ('시간', 'O'),\n",
       " ('을', 'O'),\n",
       " ('갖', 'O'),\n",
       " ('도', 'O'),\n",
       " ('록', 'O'),\n",
       " ('하', 'O'),\n",
       " ('겠습니다', 'O')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = get_bert_input_token(text, tokenizer)\n",
    "b = ner_model(**a)\n",
    "\n",
    "text_token = a['input_ids'][0][a['input_ids'][0] > 0][1:-1]\n",
    "\n",
    "label_token = b[0][1:-1].cpu()\n",
    "\n",
    "label = list([ids_to_tags[i.item()] for i in label_token])\n",
    "\n",
    "text_token = tokenizer.convert_ids_to_tokens(text_token.tolist())\n",
    "\n",
    "text_entity_dictionary = list(zip(text_token, label))\n",
    "\n",
    "text_entity_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>오늘</td>\n",
       "      <td>DT_DAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>서울대학교</td>\n",
       "      <td>OGG_EDUCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이재용</td>\n",
       "      <td>PS_NAME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>학과장</td>\n",
       "      <td>CV_POSITION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word         entity\n",
       "0     오늘          DT_DAY\n",
       "1  서울대학교   OGG_EDUCATION\n",
       "2    이재용         PS_NAME\n",
       "3     학과장    CV_POSITION"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_extractor.get_entity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '이준석, 취임 한 달 넘기고 리더십 위기?…높아지는 당내 갈등 수위'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이준석</td>\n",
       "      <td>PS_NAME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>한 달</td>\n",
       "      <td>DT_DURATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word       entity\n",
       "0   이준석      PS_NAME\n",
       "1  한 달   DT_DURATION"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_extractor.get_entity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '델타변이, 마스크도 돌파…대전 태권도장발 185명 집단감염'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>대전</td>\n",
       "      <td>LCP_CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185명</td>\n",
       "      <td>QT_MAN_COUNT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word        entity\n",
       "0    대전       LCP_CITY\n",
       "1  185명   QT_MAN_COUNT"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_extractor.get_entity(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
